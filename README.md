# ğŸ“Š DataYoti Warehouse

> **Du signal Ã  l'action** - Data Warehouse et Business Intelligence pour l'IoT environnemental

**DataYoti Warehouse** transforme les donnÃ©es IoT en insights actionnables. Cette solution complÃ¨te de Data Warehouse implÃ©mente une architecture ELT moderne avec orchestration dÃ©clarative, permettant le monitoring qualitÃ©, la traÃ§abilitÃ© et l'analyse de conformitÃ© des conditions environnementales.

## ğŸ¯ Place dans l'Ã©cosystÃ¨me DataYoti

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1ï¸âƒ£  Capteurs ESP32 (DHT22)            â”‚  â†’ datayoti-firmware
â”‚      â†“ MQTT                             â”‚
â”‚  2ï¸âƒ£  Infrastructure temps rÃ©el          â”‚  â†’ datayoti-realtime (Raspberry Pi)
â”‚      â†“ Ingestion & monitoring (OLTP)    â”‚
â”‚  3ï¸âƒ£  Data Warehouse + Analytics        â”‚  â† VOUS ÃŠTES ICI
â”‚      â†“ Dashboards & ConformitÃ© (OLAP)  â”‚
â”‚  4ï¸âƒ£  Insights actionnables              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Ce composant assure :
- ğŸ“ˆ **Analyse** des tendances environnementales sur pÃ©riodes Ã©tendues
- âœ… **VÃ©rification** de la conformitÃ© par rapport aux rÃ¨gles mÃ©tier
- ğŸ¢ **Historisation** des changements de configuration (SCD Type 2)
- ğŸ“Š **AgrÃ©gation** des mÃ©triques par site, appareil et pÃ©riode
- ğŸ“‰ **Production** de rapports de conformitÃ© dÃ©cisionnels

---

## ğŸ—ï¸ Architecture ELT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OLTP Source       â”‚  datayoti-realtime (Raspberry Pi)
â”‚  (TimescaleDB)      â”‚  (DonnÃ©es opÃ©rationnelles)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Extract & Load (Airflow)
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Layer         â”‚  RÃ©plication des donnÃ©es
â”‚  (PostgreSQL)       â”‚  brutes pour analyse
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Transform (dbt)
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Warehouse     â”‚  ModÃ¨le dimensionnel
â”‚  (Star Schema)      â”‚  pour analytics
â”‚  - 4 Dimensions     â”‚
â”‚  - 3 Faits          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Business Views     â”‚  Insights actionnables
â”‚  - Compliance       â”‚  - Dashboards
â”‚  - KPIs             â”‚  - Alertes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack technique

| Composant | Technologie | RÃ´le |
|-----------|-------------|------|
| **Orchestration** | Apache Airflow 3.1.0 | Pipelines ELT automatisÃ©s |
| **Transformation** | dbt (Data Build Tool) | Transformations SQL testables |
| **Data Warehouse** | PostgreSQL 16 | Stockage OLAP dimensionnel |
| **Task Queue** | Celery + Redis | ExÃ©cution distribuÃ©e |
| **Infrastructure** | Docker Compose | DÃ©ploiement simplifiÃ© |


---

## ğŸŒŸ FonctionnalitÃ©s clÃ©s

### Monitoring et traÃ§abilitÃ©

- ğŸ“Š **RÃ©fÃ©rentiel** : RÃ¨gles de conformitÃ© automatisÃ©es (tempÃ©rature, humiditÃ©, stabilitÃ©, disponibilitÃ©)
- ğŸ“ **TraÃ§abilitÃ©** : Journal complet des non-conformitÃ©s avec audit trail
- â±ï¸ **Historisation** : SCD Type 2 pour suivi des changements dans le temps
- ğŸ¯ **Priorisation** : Classement des risques par zone et niveau de criticitÃ©

### ModÃ¨le dimensionnel (Star Schema)

**Dimensions** :
- `dim_sites` : Sites de dÃ©ploiement (SCD2)
- `dim_devices` : Appareils IoT ESP32 (SCD2)
- `dim_dates` : Calendrier analytique (1990-2050)
- `dim_conformity_rules` : RÃ¨gles mÃ©tier (SCD2)

**Faits** (grain quotidien) :
- `fct_daily_sensor_reading` : MÃ©triques tempÃ©rature/humiditÃ©
- `fct_daily_sensor_health` : SantÃ© des capteurs (RSSI, uptime, heap)
- `fct_daily_site_compliance` : ConformitÃ© par site et par rÃ¨gle

### Pipeline dbt en 4 couches

1. **Raw** : Tables DDL pour ingestion Airflow
2. **Staging** : Nettoyage et standardisation
3. **Intermediate** : Logique mÃ©tier et SCD2
4. **Marts** : ModÃ¨le dimensionnel final

---

## ğŸš€ Installation rapide

### PrÃ©requis

- **Docker** et **Docker Compose** installÃ©s
- **4 GB RAM** minimum
- **2 CPUs** minimum
- Environnement **datayoti-realtime** en fonctionnement sur Raspberry Pi (source OLTP)

### Installation en 3 Ã©tapes

```bash
# 1. Cloner et configurer
git clone https://github.com/medkan01/datayoti-warehouse.git
cd datayoti-warehouse
cp .env.example .env
# Ã‰diter .env avec vos paramÃ¨tres

# 2. DÃ©marrer l'infrastructure
docker-compose up -d

# 3. Attendre l'initialisation (~2 min)
docker-compose logs -f airflow-init
```

### Configuration minimale (.env)

```bash
# Data Warehouse (OLAP)
DM_PG_USER=datamart_admin
DM_PG_PASSWORD=VotreMotDePasseDW123!
DM_PG_DATABASE=datayoti_datamart
DM_PG_PORT=5433

# Source OLTP (datayoti-realtime sur Raspberry Pi)
OLTP_PG_HOST=192.168.x.x  # IP du Raspberry Pi
OLTP_PG_PORT=5432
OLTP_PG_USER=mqtt_ingestor
OLTP_PG_PASSWORD=MotDePasseOLTP123!
OLTP_PG_DATABASE=datayoti_db

# Airflow
AIRFLOW_UID=50000
AIRFLOW__CORE__FERNET_KEY=<gÃ©nÃ©rer avec commande ci-dessous>
```

**GÃ©nÃ©rer la clÃ© Fernet** :
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

### Configuration Airflow

1. AccÃ©der Ã  **http://localhost:8080** (airflow / airflow)
2. Configurer les connexions (Admin â†’ Connections) :

**OLTP (source)** :
```
Connection Id: oltp_connection
Type: Postgres
Host: <OLTP_PG_HOST>
Database: <OLTP_PG_DATABASE>
Login: <OLTP_PG_USER>
Password: <OLTP_PG_PASSWORD>
Port: <OLTP_PG_PORT>
```

**OLAP (destination)** :
```
Connection Id: olap_connection
Type: Postgres
Host: datamart-db
Database: public
Login: <DM_PG_USER>
Password: <DM_PG_PASSWORD>
Port: 5432
```

3. Activer et dÃ©clencher le DAG `ingest_raw_iot_data`

---

## ğŸ“‹ Pipelines de donnÃ©es

### DAGs Airflow

| DAG | FrÃ©quence | Description |
|-----|-----------|-------------|
| `ingest_raw_iot_data` | @daily | Ingestion OLTP â†’ Raw layer |
| `dim_reference_iot` | @daily | Construction dimensions |
| `daily_facts_sensor` | @daily | AgrÃ©gation faits quotidiens |
| `dbt_housekeeping` | @weekly | Maintenance dbt |

### Exemple de requÃªte analytique

```sql
-- Taux de conformitÃ© mensuel par site
SELECT 
    s.site_name,
    d.year_number,
    d.month_name,
    ROUND(AVG(c.compliance_rate) * 100, 2) AS avg_compliance_pct,
    SUM(c.nb_critical_violations) AS total_critical_violations
FROM vw_daily_site_compliance_summary c
JOIN dim_sites s ON c.site_sk = s.site_sk AND s.is_current = TRUE
JOIN dim_dates d ON c.event_day_sk = d.date_sk
WHERE d.year_number = 2025
GROUP BY s.site_name, d.year_number, d.month_name
ORDER BY d.year_number, d.month_number, s.site_name;
```

---

## ğŸ› ï¸ DÃ©veloppement

### Commandes dbt

```bash
# AccÃ©der au conteneur
docker-compose exec airflow-scheduler bash
cd /opt/airflow/datayoti_dbt

# ExÃ©cuter les transformations
dbt run                              # Tous les modÃ¨les
dbt run --select staging.*           # Couche staging uniquement
dbt run --select +fct_daily_sensor_reading  # Avec dÃ©pendances

# Tests de qualitÃ©
dbt test                             # Tous les tests
dbt test --select dim_sites          # Tests d'un modÃ¨le

# Documentation
dbt docs generate
dbt docs serve --port 8081

# Snapshots (SCD2)
dbt snapshot

# Seeds (donnÃ©es de rÃ©fÃ©rence)
dbt seed
```

### Commandes Airflow

```bash
# Lister les DAGs
docker-compose exec airflow-scheduler airflow dags list

# DÃ©clencher manuellement
docker-compose exec airflow-scheduler airflow dags trigger ingest_raw_iot_data

# Voir les exÃ©cutions
docker-compose exec airflow-scheduler airflow dags list-runs -d ingest_raw_iot_data

# Tester une tÃ¢che
docker-compose exec airflow-scheduler airflow tasks test ingest_raw_iot_data bootstrap_raw_schema 2025-11-03
```

---

## ğŸ“Š MÃ©triques de conformitÃ©

Les rÃ¨gles sont dÃ©finies dans `datayoti_dbt/seeds/conformity_rules.csv` :

| Type de rÃ¨gle | Description | Exemple |
|---------------|-------------|---------|
| `temperature_range` | Plage acceptable | 18Â°C - 24Â°C |
| `humidity_ceiling` | Seuil maximum | â‰¤ 60% |
| `humidity_floor` | Seuil minimum | â‰¥ 35% |
| `temperature_stability` | Variation max | â‰¤ 3Â°C |
| `uptime_ratio` | DisponibilitÃ© min | â‰¥ 95% |

Chaque rÃ¨gle a un **niveau de criticitÃ©** (LOW, MEDIUM, HIGH) pour prioriser les alertes.

---

## ğŸ” Interfaces disponibles

| Interface | URL | Credentials | Usage |
|-----------|-----|-------------|-------|
| **Airflow** | http://localhost:8080 | airflow / airflow | Orchestration |
| **Flower** | http://localhost:5555 | - | Monitoring Celery |
| **PostgreSQL DW** | localhost:5433 | voir `.env` | Connexion directe |
| **dbt Docs** | http://localhost:8081 | - | Documentation (si actif) |

---

## ï¿½ DÃ©pannage

### Erreur "Connection refused" sur OLTP

```bash
# VÃ©rifier connectivitÃ©
ping <OLTP_PG_HOST>

# Tester connexion PostgreSQL
psql -h <OLTP_PG_HOST> -p <OLTP_PG_PORT> -U <OLTP_PG_USER> -d <OLTP_PG_DATABASE>
```

### DAGs non visibles

```bash
# Logs du dag-processor
docker-compose logs airflow-dag-processor

# VÃ©rifier syntaxe Python
docker-compose exec airflow-scheduler python /opt/airflow/dags/<dag_file>.py
```

### dbt : "Database Error"

```bash
# Tester configuration dbt
docker-compose exec airflow-scheduler bash
cd /opt/airflow/datayoti_dbt
dbt debug
```

**VÃ©rifier** :
- `host: datamart-db` (nom du service Docker)
- `port: 5432` (port interne conteneur)
- Credentials dans `profiles/profiles.yml`

---

## ğŸ“ Structure simplifiÃ©e

```
datayoti-warehouse/
â”œâ”€â”€ docker-compose.yml              # Infrastructure
â”œâ”€â”€ .env.example                    # Template config
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                       # Pipelines Airflow
â”‚   â””â”€â”€ config/                     # Configuration
â””â”€â”€ datayoti_dbt/
    â”œâ”€â”€ models/                     # Transformations dbt
    â”‚   â”œâ”€â”€ raw/                    # Couche 1: DDL
    â”‚   â”œâ”€â”€ staging/                # Couche 2: Nettoyage
    â”‚   â”œâ”€â”€ intermediate/           # Couche 3: Logique mÃ©tier
    â”‚   â””â”€â”€ marts/                  # Couche 4: Star Schema
    â”œâ”€â”€ seeds/                      # DonnÃ©es rÃ©fÃ©rence
    â”‚   â””â”€â”€ conformity_rules.csv
    â”œâ”€â”€ snapshots/                  # Historisation SCD2
    â””â”€â”€ profiles/                   # Connexion DB
```

---

## ï¿½ SÃ©curitÃ©

âš ï¸ **En production** :
1. Changer **tous** les mots de passe par dÃ©faut
2. RÃ©gÃ©nÃ©rer la clÃ© Fernet Airflow
3. Ne **jamais** commiter `.env`
4. Utiliser des secrets managers (Vault, AWS Secrets)
5. Activer SSL/TLS pour PostgreSQL
6. Configurer des backups rÃ©guliers

---

## ï¿½ Ressources

- ğŸ“– [Apache Airflow](https://airflow.apache.org/docs/)
- ğŸ“– [dbt Documentation](https://docs.getdbt.com/)
- ğŸ“– [Kimball Dimensional Modeling](https://www.kimballgroup.com/)
- ğŸ”— [Firmware ESP32](../datayoti-firmware)
- ğŸ”— [Infrastructure temps rÃ©el](../datayoti-realtime)

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ‘¨â€ï¿½ Contact

- **LinkedIn** : [Mehdi Akniou](https://linkedin.com/in/mehdi-akniou)
- **Email** : contact@mehdi-akniou.com
- **GitHub** : [@medkan01](https://github.com/medkan01)

---

**DataYoti Warehouse** - Du signal Ã  l'action ğŸ“Š

*Data Warehouse et Business Intelligence pour l'IoT environnemental*